# =============================================================================
# dgx-vllm — Single Source of Truth for All Versions and Build Config
# =============================================================================
#
# This file drives the entire build pipeline. build.sh and push.sh source it
# automatically. The Dockerfile reads key values via ARG/--build-arg.
#
# To build:   ./build.sh
# To push:    ./push.sh
# To verify:  docker exec <container> pip show <package>
#
# WARNING: All patches in this repo are tested against these exact versions.
# Changing any version may break patches — test thoroughly after updates.
#
# =============================================================================

# --- Build Identity ---
IMAGE_VERSION=22
IMAGE_NAME=dgx-vllm
DOCKER_HUB_REPO=avarok/dgx-vllm-nvfp4-kernel

# --- Base Image ---
BASE_IMAGE=nvidia/cuda:13.0.2-cudnn-devel-ubuntu24.04

# --- Python ---
PYTHON_VERSION=3.12.3

# --- PyTorch (pinned to cu130 index) ---
TORCH_VERSION=2.10.0+cu130
TORCHVISION_VERSION=0.25.0+cu130
TORCHAUDIO_VERSION=2.10.0+cu130

# --- vLLM (built from source) ---
# This is the exact commit the Dockerfile checks out and builds.
# Patches in fix_*.py and integrate_*.sh target this revision.
VLLM_VERSION=0.16.0rc2.dev236
VLLM_COMMIT=3b30e6150777de549b11f67dde3ecc0d3b1f3f50

# --- FlashInfer ---
# Patched files: cutlass/include/cutlass/float_subbyte.h (E2M1 SM121 fix)
FLASHINFER_VERSION=0.6.3

# --- Triton ---
# Patched files: backends/nvidia/include/cuda_fp4.h
TRITON_VERSION=3.6.0

# --- XGrammar ---
XGRAMMAR_VERSION=0.1.29

# --- CUTLASS (multiple vendored copies inside pip packages) ---
# flashmla embeds CUTLASS for MLA kernels
CUTLASS_FLASHMLA_VERSION=4.3.5
# qutlass embeds CUTLASS for quantized kernels
CUTLASS_QUTLASS_VERSION=4.1.0
# vllm-flash-attn embeds CUTLASS for attention
CUTLASS_FLASH_ATTN_VERSION=3.8.0
# cutlass-src is the main CUTLASS source used for FP4 MoE GEMM
CUTLASS_SRC_VERSION=4.2.1

# --- NVIDIA Libraries ---
NCCL_VERSION=2.28.9
NVIDIA_CUTLASS_DSL_VERSION=4.4.0

# --- HuggingFace ---
TRANSFORMERS_VERSION=4.57.6
TOKENIZERS_VERSION=0.22.2

# --- CUDA ---
CUDA_VERSION=13.0.2

# =============================================================================
# Patched Files (relative to site-packages or container paths)
# =============================================================================
#
# These files are modified by our build-time or runtime patches. If any
# upstream package updates these files, the patches may need updating.
#
# FlashInfer E2M1 fix (fix_flashinfer_e2m1_sm121.py):
#   flashinfer/data/cutlass/include/cutlass/float_subbyte.h
#
# Triton FP4 type fix (patch applied at build time):
#   triton/backends/nvidia/include/cuda_fp4.h
#
# NVIDIA CUDA FP4 type fix (patch applied at build time):
#   nvidia/cu13/include/cuda_fp4.h
#
# vLLM modelopt quantization (fix_flashinfer_nvfp4_moe_backend.py):
#   vllm/model_executor/layers/quantization/modelopt.py
#
# vLLM NVFP4 emulation utils (patch_nvfp4_utils_sw_e2m1.py):
#   vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py
#
# =============================================================================
